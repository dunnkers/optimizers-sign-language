{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing optimizers in a sign language prediction application\n",
    "Deep Learning (20/21) `WMAI017-05.2020-2021.2A` - by Jeroen Overschie and Loran Knol.\n",
    "\n",
    "Uses [data](https://www.kaggle.com/grassknoted/asl-alphabet) describing the ASL alphabet and tries to classify the images correctly using an adapted custom Neural Network, built with TensorFlow/Keras. Runs the model fitting multiple times for various optimizers, such that we can compare various optimizers against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.15.5\n2.1.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "a5f1f752-8365-40a8-a849-8ab2ab34db63",
    "_kg_hide-output": true,
    "_uuid": "a334a3525c03f23a55668f55db390abca81eab20"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/asl_alphabet_train/asl_alphabet_train\"\n",
    "target_size = (64, 64)\n",
    "target_dims = (64, 64, 3)\n",
    "n_classes = 29\n",
    "val_frac = 0.1\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6106ce0-3618-43c6-bf05-a8e64fb4cbdd",
    "_uuid": "3c0043a20407434fa301f51565bb184b4b54cde3"
   },
   "source": [
    "Use ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 78300 images belonging to 29 classes.\n",
      "Found 8700 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "data_augmentor = ImageDataGenerator(samplewise_center=True, \n",
    "                                    samplewise_std_normalization=True, \n",
    "                                    validation_split=val_frac)\n",
    "train_generator = data_augmentor.flow_from_directory(data_dir, target_size=target_size, batch_size=batch_size, shuffle=True, subset=\"training\")\n",
    "val_generator = data_augmentor.flow_from_directory(data_dir, target_size=target_size, batch_size=batch_size, subset=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15dc1ae0-bc4a-4988-9c8c-01a9f8b5524c",
    "_uuid": "1fab4c91abce8e388652ef9c4c3d3ed6a3489a7c"
   },
   "source": [
    "## Define the model\n",
    "Define custom Neural Network, see [link](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu). Convolutes the images, activates using ReLU and performes Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "435128bc-b3cb-410e-85a7-3ef43748b3f8",
    "_uuid": "dc201ab00d5790a06511a86563920f90a3f8f2d5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:507: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3831: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:126: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3138: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential()\n",
    "my_model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=target_dims))\n",
    "my_model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\n",
    "my_model.add(Dropout(0.5))\n",
    "my_model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n",
    "my_model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n",
    "my_model.add(Dropout(0.5))\n",
    "my_model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\n",
    "my_model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\n",
    "my_model.add(Flatten())\n",
    "my_model.add(Dropout(0.5))\n",
    "my_model.add(Dense(512, activation='relu'))\n",
    "my_model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, define a custom _Callback_ that also saves the time spent per epoch. Inherits from `CSVLogger` so also saves the loss per epoch to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "class EpochLoss(CSVLogger):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "        super(EpochLoss, self).on_train_begin()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "        super(EpochLoss, self).on_epoch_begin(epoch, logs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['epoch_time'] = time.time() - self.epoch_time_start\n",
    "        super(EpochLoss, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "Configure a line-up of optimizers to use in the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Yogi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "# if you use tf.keras:\n",
    "from keras import backend as K  \n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "# if you use standalone Keras\n",
    "# from keras import backend as K\n",
    "# from keras.optimizers import Optimizer\n",
    "    \n",
    "class Yogi(Optimizer):\n",
    "    \"\"\"Yogi optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    Arguments:\n",
    "      lr: float >= 0. Learning rate.\n",
    "      beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "      beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "      decay: float >= 0. Learning rate decay over each update.\n",
    "      amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "          algorithm from the paper \"On the Convergence of Adam and\n",
    "          Beyond\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               lr=0.001,\n",
    "               beta_1=0.9,\n",
    "               beta_2=0.999,\n",
    "               epsilon=None,\n",
    "               decay=0.00000001,\n",
    "               amsgrad=False,\n",
    "               **kwargs):\n",
    "        super(Yogi, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [state_ops.assign_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (  # pylint: disable=g-no-augmented-assignment\n",
    "                1. / (1. + self.decay * math_ops.cast(self.iterations,\n",
    "                                                    K.dtype(self.decay))))\n",
    "\n",
    "        t = math_ops.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (\n",
    "            K.sqrt(1. - math_ops.pow(self.beta_2, t)) /\n",
    "            (1. - math_ops.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            #v_t = (self.beta_2 * v) + (1. - self.beta_2) * math_ops.square(g) # from amsgrad\n",
    "            v_t = v - (1-self.beta_2)*K.sign(v-math_ops.square(g))*math_ops.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(state_ops.assign(m, m_t))\n",
    "            self.updates.append(state_ops.assign(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(state_ops.assign(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'lr': float(K.get_value(self.lr)),\n",
    "            'beta_1': float(K.get_value(self.beta_1)),\n",
    "            'beta_2': float(K.get_value(self.beta_2)),\n",
    "            'decay': float(K.get_value(self.decay)),\n",
    "            'epsilon': self.epsilon,\n",
    "            'amsgrad': self.amsgrad\n",
    "            }\n",
    "        base_config = super(Yogi, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.training import optimizer\n",
    "\n",
    "class RAdamOptimizer(optimizer.Optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    RAdam optimizer : On The Variance Of The Adaptive Learning Rate And Beyond\n",
    "    https://arxiv.org/abs/1908.03265\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate=0.001,\n",
    "                 beta1=0.9,\n",
    "                 beta2=0.999,\n",
    "                 epsilon=1e-8,\n",
    "                 weight_decay=0.,\n",
    "                 use_locking=False,\n",
    "                 name=\"RAdam\"):\n",
    "\n",
    "        super(RAdamOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "        self._weight_decay = weight_decay\n",
    "\n",
    "        self._lr_t = None\n",
    "        self._step_t = None\n",
    "        self._beta1_t = None\n",
    "        self._beta2_t = None\n",
    "        self._epsilon_t = None\n",
    "        self._weight_decay_t = None\n",
    "\n",
    "    def _get_beta_accumulators(self):\n",
    "        with ops.init_scope():\n",
    "            if context.executing_eagerly():\n",
    "                graph = None\n",
    "            else:\n",
    "                graph = ops.get_default_graph()\n",
    "            return (self._get_non_slot_variable(\"step\", graph=graph),\n",
    "                    self._get_non_slot_variable(\"beta1_power\", graph=graph),\n",
    "                    self._get_non_slot_variable(\"beta2_power\", graph=graph))\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        first_var = min(var_list, key=lambda x: x.name)\n",
    "        self._create_non_slot_variable(initial_value=1.0, name=\"step\", colocate_with=first_var)\n",
    "        self._create_non_slot_variable(initial_value=self._beta1, name=\"beta1_power\", colocate_with=first_var)\n",
    "        self._create_non_slot_variable(initial_value=self._beta2, name=\"beta2_power\", colocate_with=first_var)\n",
    "\n",
    "        for v in var_list:\n",
    "            self._zeros_slot(v, \"m\", self._name)\n",
    "            self._zeros_slot(v, \"v\", self._name)\n",
    "\n",
    "    def _prepare(self):\n",
    "        lr = self._lr\n",
    "        beta1 = self._beta1\n",
    "        beta2 = self._beta2\n",
    "        epsilon = self._epsilon\n",
    "        weight_decay = self._weight_decay\n",
    "\n",
    "        self._lr_t = ops.convert_to_tensor(lr, name=\"learning_rate\")\n",
    "        self._beta1_t = ops.convert_to_tensor(beta1, name=\"beta1\")\n",
    "        self._beta2_t = ops.convert_to_tensor(beta2, name=\"beta2\")\n",
    "        self._epsilon_t = ops.convert_to_tensor(epsilon, name=\"epsilon\")\n",
    "        self._weight_decay_t = ops.convert_to_tensor(weight_decay, name=\"weight_decay\")\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        return self._resource_apply_dense(grad, var)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        step, beta1_power, beta2_power = self._get_beta_accumulators()\n",
    "        step = math_ops.cast(step, var.dtype.base_dtype)\n",
    "        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power)\n",
    "\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_t = state_ops.assign(m, beta1_t * m + (1.0 - beta1_t) * grad, use_locking=self._use_locking)\n",
    "        mhat_t = m_t / (1.0 - beta1_power)\n",
    "\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_t = state_ops.assign(v, beta2_t * v + (1.0 - beta2_t) * math_ops.square(grad), use_locking=self._use_locking)\n",
    "        vhat_t = math_ops.sqrt(v_t / ((1.0 - beta2_power) + epsilon_t))\n",
    "\n",
    "        r_t = math_ops.sqrt( ((sma_t - 4.0) * (sma_t - 2.0) * sma_inf) / ((sma_inf - 4.0) * (sma_inf - 2.0) * sma_t) )\n",
    "\n",
    "        var_t = tf.cond(sma_t >= 5.0, lambda : r_t * mhat_t / (vhat_t + epsilon_t), lambda : mhat_t)\n",
    "\n",
    "        if self._weight_decay > 0.0:\n",
    "            var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype) * var\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, lr_t * var_t, use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "\n",
    "        return control_flow_ops.group(*updates)\n",
    "\n",
    "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
    "        step, beta1_power, beta2_power = self._get_beta_accumulators()\n",
    "        step = math_ops.cast(step, var.dtype.base_dtype)\n",
    "        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * step * beta2_power / (1.0 - beta2_power)\n",
    "\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n",
    "\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = scatter_add(m, indices, m_scaled_g_values)\n",
    "\n",
    "        mhat_t = m_t / (1.0 - beta1_power)\n",
    "\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n",
    "\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        vhat_t = math_ops.sqrt(v_t / (1.0 - beta2_power) + epsilon_t)\n",
    "\n",
    "        r_t = math_ops.sqrt( ((sma_t - 4.0) * (sma_t - 2.0) * sma_inf) / ((sma_inf - 4.0) * (sma_inf - 2.0) * sma_t) )\n",
    "\n",
    "        var_t = tf.cond(sma_t >= 5.0, lambda : r_t * mhat_t / (vhat_t + epsilon_t), lambda : mhat_t)\n",
    "\n",
    "        if self._weight_decay > 0.0:\n",
    "            var_t += math_ops.cast(self._weight_decay_t, var.dtype.base_dtype) * var\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, lr_t * var_t, use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "\n",
    "        return control_flow_ops.group(*updates)\n",
    "\n",
    "    def _apply_sparse(self, grad, var):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad.values,\n",
    "            var,\n",
    "            grad.indices,\n",
    "            lambda x, i, v: state_ops.scatter_add(x, i, v, use_locking=self._use_locking))\n",
    "\n",
    "    def _resource_scatter_add(self, x, i, v):\n",
    "        with ops.control_dependencies([resource_variable_ops.resource_scatter_add(x.handle, i, v)]):\n",
    "            return x.value()\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        return self._apply_sparse_shared(grad, var, indices, self._resource_scatter_add)\n",
    "\n",
    "    def _finish(self, update_ops, name_scope):\n",
    "        with ops.control_dependencies(update_ops):\n",
    "            step, beta1_power, beta2_power = self._get_beta_accumulators()\n",
    "            with ops.colocate_with(beta1_power):\n",
    "                update_step = step.assign(step + 1.0, use_locking=self._use_locking)\n",
    "                update_beta1 = beta1_power.assign(beta1_power * self._beta1_t, use_locking=self._use_locking)\n",
    "                update_beta2 = beta2_power.assign(beta2_power * self._beta2_t, use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*update_ops + [update_step, update_beta1, update_beta2], name=name_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "AdaBelief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"AdaBelief for TensorFlow.\n",
    "Modified from tensorflow/tensorflow/python/training/adam.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.training import optimizer\n",
    "from tensorflow.python.training import training_ops\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "\n",
    "@tf_export(v1=[\"train.AdaBeliefOptimizer\"])\n",
    "class AdaBeliefOptimizer(optimizer.Optimizer):\n",
    "    \"\"\"Optimizer that implements the Adam algorithm.\n",
    "    References:\n",
    "    Adam - A Method for Stochastic Optimization:\n",
    "      [Kingma et al., 2015](https://arxiv.org/abs/1412.6980)\n",
    "      ([pdf](https://arxiv.org/pdf/1412.6980.pdf))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               learning_rate=0.001,\n",
    "               beta1=0.9,\n",
    "               beta2=0.999,\n",
    "               epsilon=1e-8,\n",
    "               use_locking=False,\n",
    "               name=\"AdaBelief\", amsgrad = False):\n",
    "        r\"\"\"Construct a new Adam optimizer.\n",
    "        Initialization:\n",
    "        $$m_0 := 0 \\text{(Initialize initial 1st moment vector)}$$\n",
    "        $$v_0 := 0 \\text{(Initialize initial 2nd moment vector)}$$\n",
    "        $$t := 0 \\text{(Initialize timestep)}$$\n",
    "        The update rule for `variable` with gradient `g` uses an optimization\n",
    "        described at the end of section 2 of the paper:\n",
    "        $$t := t + 1$$\n",
    "        $$\\text{lr}_t := \\mathrm{learning_rate} *\n",
    "          \\sqrt{1 - \\beta_2^t} / (1 - \\beta_1^t)$$\n",
    "        $$m_t := \\beta_1 * m_{t-1} + (1 - \\beta_1) * g$$\n",
    "        $$v_t := \\beta_2 * v_{t-1} + (1 - \\beta_2) * g * g$$\n",
    "        $$\\text{variable} := \\text{variable} -\n",
    "          \\text{lr}_t * m_t / (\\sqrt{v_t} + \\epsilon)$$\n",
    "        The default value of 1e-8 for epsilon might not be a good default in\n",
    "        general. For example, when training an Inception network on ImageNet a\n",
    "        current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\n",
    "        formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
    "        the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
    "        hat\" in the paper.\n",
    "        The sparse implementation of this algorithm (used when the gradient is an\n",
    "        IndexedSlices object, typically because of `tf.gather` or an embedding\n",
    "        lookup in the forward pass) does apply momentum to variable slices even if\n",
    "        they were not used in the forward pass (meaning they have a gradient equal\n",
    "        to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
    "        accumulator. This means that the sparse behavior is equivalent to the dense\n",
    "        behavior (in contrast to some momentum implementations which ignore momentum\n",
    "        unless a variable slice was actually used).\n",
    "        Args:\n",
    "          learning_rate: A Tensor or a floating point value.  The learning rate.\n",
    "          beta1: A float value or a constant float tensor. The exponential decay\n",
    "            rate for the 1st moment estimates.\n",
    "          beta2: A float value or a constant float tensor. The exponential decay\n",
    "            rate for the 2nd moment estimates.\n",
    "          epsilon: A small constant for numerical stability. This epsilon is\n",
    "            \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "            Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "          use_locking: If True use locks for update operations.\n",
    "          name: Optional name for the operations created when applying gradients.\n",
    "            Defaults to \"Adam\".\n",
    "        @compatibility(eager)\n",
    "        When eager execution is enabled, `learning_rate`, `beta1`, `beta2`, and\n",
    "        `epsilon` can each be a callable that takes no arguments and returns the\n",
    "        actual value to use. This can be useful for changing these values across\n",
    "        different invocations of optimizer functions.\n",
    "        @end_compatibility\n",
    "        \"\"\"\n",
    "        super(AdaBeliefOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "        # Tensor versions of the constructor arguments, created in _prepare().\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = None\n",
    "        self._beta2_t = None\n",
    "        self._epsilon_t = None\n",
    "\n",
    "    def _get_beta_accumulators(self):\n",
    "        with ops.init_scope():\n",
    "          if context.executing_eagerly():\n",
    "            graph = None\n",
    "          else:\n",
    "            graph = ops.get_default_graph()\n",
    "          return (self._get_non_slot_variable(\"beta1_power\", graph=graph),\n",
    "                  self._get_non_slot_variable(\"beta2_power\", graph=graph))\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        # Create the beta1 and beta2 accumulators on the same device as the first\n",
    "        # variable. Sort the var_list to make sure this device is consistent across\n",
    "        # workers (these need to go on the same PS, otherwise some updates are\n",
    "        # silently ignored).\n",
    "        first_var = min(var_list, key=lambda x: x.name)\n",
    "        self._create_non_slot_variable(\n",
    "            initial_value=self._beta1, name=\"beta1_power\", colocate_with=first_var)\n",
    "        self._create_non_slot_variable(\n",
    "            initial_value=self._beta2, name=\"beta2_power\", colocate_with=first_var)\n",
    "\n",
    "        # Create slots for the first and second moments.\n",
    "        for v in var_list:\n",
    "          self._zeros_slot(v, \"m\", self._name)\n",
    "          self._zeros_slot(v, \"v\", self._name)\n",
    "          self._zeros_slot(v, \"vhat\", self._name)\n",
    "\n",
    "    def _prepare(self):\n",
    "        lr = self._lr\n",
    "        beta1 = self._beta1\n",
    "        beta2 = self._beta2\n",
    "        epsilon = self._epsilon\n",
    "\n",
    "        self._lr_t = ops.convert_to_tensor(lr, name=\"learning_rate\")\n",
    "        self._beta1_t = ops.convert_to_tensor(beta1, name=\"beta1\")\n",
    "        self._beta2_t = ops.convert_to_tensor(beta2, name=\"beta2\")\n",
    "        self._epsilon_t = ops.convert_to_tensor(epsilon, name=\"epsilon\")\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "          graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "          beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), var.dtype.base_dtype)\n",
    "          beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
    "          lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "\n",
    "          beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "          beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "          epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "\n",
    "          step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "\n",
    "          # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "          m = self.get_slot(var, \"m\")\n",
    "          m_scaled_g_values = grad * (1 - beta1_t)\n",
    "          m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "          # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "          v = self.get_slot(var, \"v\")\n",
    "          v_scaled_g_values = (grad -m_t) * (grad - m_t) * (1 - beta2_t)\n",
    "          v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values + epsilon_t, use_locking=self._use_locking)\n",
    "\n",
    "          # amsgrad\n",
    "          vhat = self.get_slot(var, \"vhat\")\n",
    "          if self.amsgrad:\n",
    "              vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "              v_sqrt = math_ops.sqrt(vhat_t)\n",
    "          else:\n",
    "              vhat_t = state_ops.assign(vhat, vhat)\n",
    "              v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "          # Compute the bounds\n",
    "          step_size = step_size / (v_sqrt + epsilon_t)\n",
    "          bounded_lr = m_t * step_size\n",
    "\n",
    "          var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "          return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "          graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "          beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), grad.dtype.base_dtype)\n",
    "          beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), grad.dtype.base_dtype)\n",
    "          lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n",
    "          beta1_t = math_ops.cast(self._beta1_t, grad.dtype.base_dtype)\n",
    "          beta2_t = math_ops.cast(self._beta2_t, grad.dtype.base_dtype)\n",
    "          epsilon_t = math_ops.cast(self._epsilon_t, grad.dtype.base_dtype)\n",
    "\n",
    "          step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "\n",
    "          # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "          m = self.get_slot(var, \"m\")\n",
    "          m_scaled_g_values = grad * (1 - beta1_t)\n",
    "          m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "          # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "          v = self.get_slot(var, \"v\")\n",
    "          v_scaled_g_values = (grad - m_t) * (grad - m_t) * (1 - beta2_t)\n",
    "          v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values + epsilon_t, use_locking=self._use_locking)\n",
    "\n",
    "          # amsgrad\n",
    "          vhat = self.get_slot(var, \"vhat\")\n",
    "          if self.amsgrad:\n",
    "              vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "              v_sqrt = math_ops.sqrt(vhat_t)\n",
    "          else:\n",
    "              vhat_t = state_ops.assign(vhat, vhat)\n",
    "              v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "          # Compute the bounds\n",
    "          step_size = step_size / (v_sqrt + epsilon_t)\n",
    "          bounded_lr = m_t * step_size\n",
    "\n",
    "          var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "\n",
    "          return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
    "        beta1_power, beta2_power = self._get_beta_accumulators()\n",
    "        beta1_power = math_ops.cast(beta1_power, var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(beta2_power, var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "        lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = scatter_add(m, indices, m_scaled_g_values)\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad - m_t) * (grad - m_t) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = scatter_add(v, indices, v_scaled_g_values + epsilon_t)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self.amsgrad:\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "        var_update = state_ops.assign_sub(\n",
    "            var, lr * m_t / (v_sqrt + epsilon_t), use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse(self, grad, var):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad.values,\n",
    "            var,\n",
    "            grad.indices,\n",
    "            lambda x, i, v: state_ops.scatter_add(  # pylint: disable=g-long-lambda\n",
    "                x,\n",
    "                i,\n",
    "                v,\n",
    "                use_locking=self._use_locking))\n",
    "\n",
    "    def _resource_scatter_add(self, x, i, v):\n",
    "        with ops.control_dependencies(\n",
    "            [resource_variable_ops.resource_scatter_add(x.handle, i, v)]):\n",
    "            return x.value()\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        return self._apply_sparse_shared(grad, var, indices,\n",
    "                                         self._resource_scatter_add)\n",
    "\n",
    "    def _finish(self, update_ops, name_scope):\n",
    "        # Update the power accumulators.\n",
    "        with ops.control_dependencies(update_ops):\n",
    "            beta1_power, beta2_power = self._get_beta_accumulators()\n",
    "            with ops.colocate_with(beta1_power):\n",
    "                update_beta1 = beta1_power.assign(\n",
    "                               beta1_power * self._beta1_t, use_locking=self._use_locking)\n",
    "                update_beta2 = beta2_power.assign(\n",
    "                                beta2_power * self._beta2_t, use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(\n",
    "            *update_ops + [update_beta1, update_beta2], name=name_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'customnet'\n",
    "experiments = {\n",
    "    'customnet': '0.001 LR, custom Neural Network.'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adadelta, Adagrad, Adam, Adamax, Nadam, RMSprop, SGD\n",
    "\n",
    "lr = 0.001\n",
    "optimizers = [\n",
    "    ('Yogi', Yogi(lr=lr)),\n",
    "    ('AdaBelief', AdaBeliefOptimizer(learning_rate=lr)),\n",
    "    ('RAdam', RAdamOptimizer(learning_rate=lr)),\n",
    "    ('Adadelta', Adadelta(lr=lr)),\n",
    "    ('Adagrad', Adagrad(lr=lr)),\n",
    "    ('Adam', Adam(lr=lr)),\n",
    "    ('Adamax', Adamax(lr=lr)),\n",
    "    ('Nadam', Nadam(lr=lr)),\n",
    "    ('RMSprop', RMSprop(lr=lr)),\n",
    "    ('SGD', SGD(lr=lr, momentum=0.9, nesterov=True))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:167: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:179: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:183: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:192: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'lr': 0.0010000000474974513, 'rho': 0.95, 'decay': 0.0, 'epsilon': 1e-07}"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "name, optim = optimizers[3]\n",
    "optim.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save optimizer configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved Yogi config to models/customnet_Yogi/optimizer.json ✓\n",
      "Saved AdaBelief config to models/customnet_AdaBelief/optimizer.json ✓\n",
      "Saved RAdam config to models/customnet_RAdam/optimizer.json ✓\n",
      "Saved Adadelta config to models/customnet_Adadelta/optimizer.json ✓\n",
      "Saved Adagrad config to models/customnet_Adagrad/optimizer.json ✓\n",
      "Saved Adam config to models/customnet_Adam/optimizer.json ✓\n",
      "Saved Adamax config to models/customnet_Adamax/optimizer.json ✓\n",
      "Saved Nadam config to models/customnet_Nadam/optimizer.json ✓\n",
      "Saved RMSprop config to models/customnet_RMSprop/optimizer.json ✓\n",
      "Saved SGD config to models/customnet_SGD/optimizer.json ✓\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "for name, optimizer in optimizers:\n",
    "    out_dir = os.path.join('models', f'{experiment}_{name}')\n",
    "    if not os.path.exists(out_dir): os.makedirs(out_dir)\n",
    "    \n",
    "    optim_path = os.path.join(out_dir, 'optimizer.json')\n",
    "    try:\n",
    "        conf = getattr(optimizer, 'get_config', lambda: {})()\n",
    "    except:\n",
    "        conf = {}\n",
    "    obj = pd.Series(conf)\n",
    "    obj['name'] = name\n",
    "    obj['learning_rate'] = lr\n",
    "    obj.to_json(optim_path)\n",
    "    print(f'Saved {name} config to {optim_path} ✓')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d8be6e96-7e26-4cf9-b8a8-f4a7074be4bf",
    "_uuid": "254cf9f24aecdc1150f9dbb5e47e53d15362320c"
   },
   "source": [
    "## Training\n",
    "Train model, once on every optimizer. Save model once trained; without the optimizer, however. To load the model again, we will have to again compile the model using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "545599f9-c500-40b9-9853-22105572d82e",
    "_uuid": "655cfaa648459d757a204d81269395591771c5fb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Executing Yogi...\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3008: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/s2995697/Documents/git/deep-learning/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "   3/1224 [..............................] - ETA: 1:58:06 - loss: 6.7605 - acc: 0.0729"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-55a16dcac408>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEpochLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/deep-learning/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name, optimizer in optimizers:\n",
    "    print(f'Executing {name}...')\n",
    "    out_dir = os.path.join('models', f'{experiment}_{name}')\n",
    "    loss_path = os.path.join(out_dir, 'epoch_loss.csv')\n",
    "    model_path = os.path.join(out_dir, 'model.h5')\n",
    "    try:\n",
    "        my_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "        my_model.fit_generator(train_generator, epochs=10, validation_data=val_generator, callbacks=[EpochLoss(loss_path)])\n",
    "        my_model.save(model_path, include_optimizer=False)\n",
    "    except Exception as e:\n",
    "        print(optimizer, 'failed', e)\n",
    "print('All models ran ✨')"
   ]
  },
  {
   "source": [
    "## Save full model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "my_model = tf.keras.models.load_model('../models/kaggle_Adam/model.h5')\n",
    "my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "my_model.save('../demo/public/adam.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "fc6698a572f044efe7896495e6e801ab0305861dfc7c47aead588f0c1b626ffb"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}